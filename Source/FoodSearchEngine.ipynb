{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  You gotta run the following sections in order for it to work:\n",
    "#  Libraries Imports --> Classes --> Main Declarations --> Initialization --> Searching\n",
    "\n",
    "# Main entry point has the urls that have been used as the main initial seeds of data \n",
    "# for the crawler and for the whole engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for mathmatical operations\n",
    "import nltk # for language processing purposes\n",
    "import requests # for doing http requests\n",
    "import re # for regular expressions usage\n",
    "from bs4 import BeautifulSoup # For extracting infomration from raw html\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # for tokenizing sentences and words\n",
    "#from nltk.stem import ISRIStemmer as ArStemmer # Arabic stemmer\n",
    "from nltk.stem import PorterStemmer as EnStemmer # English stemmer\n",
    "from nltk.corpus import stopwords # for latest conventional stop words\n",
    "from nltk.corpus import wordnet as wn # for getting synonyms of a word and finding similarities\n",
    "import time # for time measurements\n",
    "import sklearn # for data splitting and model training\n",
    "import pandas as pd # for reading \n",
    "import csv # for writing\n",
    "from sklearn.model_selection import train_test_split # for splitting data into test and training portions\n",
    "from sklearn.linear_model import LinearRegression # for creating a learn regresssion model\n",
    "from sklearn.tree import DecisionTreeRegressor # for creating a desicion tree regressor model\n",
    "import hashlib # for getting the hash of a string\n",
    "from joblib import load, dump # for loading and saving trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Documents = list()\n",
    "Index = dict()\n",
    "ModelInfo = list()\n",
    "englishStopwords = Helper().AddMoreStopWords(stopwords.words(\"english\"))\n",
    "enStemmer = EnStemmer()\n",
    "\n",
    "documentsPath = \"foodSearchEngineDocuments.txt\"\n",
    "indexPath = \"foodSearchEngineIndex.txt\"\n",
    "mlModelPath = \"foodSearchEngineMlModel.txt\"\n",
    "traningModelDataPath = \"foodSearchEngineTrainingData.csv\"\n",
    "trainedModelPath = \"rankerModel.joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Documents = eval(FileManager().Load(documentsPath))\n",
    "Index = eval(FileManager().Load(indexPath))\n",
    "ModelInfo = eval(FileManager().Load(mlModelPath))\n",
    "Model = FileManager().LoadRankerModel(trainedModelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class for some general usage functions\n",
    "class Helper:\n",
    "    def AddMoreStopWords(self, stopwords):\n",
    "        stopwords.append(\".\")\n",
    "        stopwords.append(\"!\")\n",
    "        stopwords.append(\"%\")\n",
    "        stopwords.append(\"#\")\n",
    "        stopwords.append(\"$\")\n",
    "        stopwords.append(\"^\")\n",
    "        stopwords.append(\"*\")\n",
    "        stopwords.append(\"&\")\n",
    "        stopwords.append(\"?\")\n",
    "        stopwords.append(\",\")\n",
    "        stopwords.append(\":\")\n",
    "        stopwords.append(\";\")\n",
    "        stopwords.append(\"(\")\n",
    "        stopwords.append(\")\")\n",
    "        stopwords.append(\"[\")\n",
    "        stopwords.append(\"]\")\n",
    "        \n",
    "        for i in range(10):\n",
    "            stopwords.append(str(i))\n",
    "        return stopwords\n",
    "    \n",
    "    def CreateDataFrame(self, data, forPrediction = True):\n",
    "        # define the columns\n",
    "        if (forPrediction == False):\n",
    "            columns = ['query','doc_tf','url_tf','doc_idf','url_idf','doc_tf-idf','url_tf-idf','url_length','doc_length', 'page_rank', 'outgoing_links']\n",
    "        else:\n",
    "            columns = ['query','doc_tf','url_tf','doc_idf','url_idf','doc_tf-idf','url_tf-idf','url_length','doc_length', 'outgoing_links']\n",
    "        \n",
    "        # Read the data using the columns as a dataframe\n",
    "        return pd.DataFrame([[data[i][c] for c in columns] for i in range(len(data))], columns= columns)\n",
    "        \n",
    "        \n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #\n",
    "\n",
    "# Cralwer class for retrieving urls of a specific website \n",
    "class Crawler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ToVisit = list()\n",
    "        self.Visited = set()\n",
    "        \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    # Retrieves the relevent text from a raw html string\n",
    "    def TextFromHtml(self, html):\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        # get ride of all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text\n",
    "        \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def Fetch(self, url):\n",
    "        print ('Fetching... ', url)\n",
    "        try:\n",
    "            return requests.get(url).content\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def GetCurrentUrlToFetch(self):\n",
    "        res = self.ToVisit.pop()\n",
    "        \n",
    "        while res in self.Visited:\n",
    "            if (len(self.ToVisit) < 1 and res in self.Visited):\n",
    "                return \"\"\n",
    "            res = self.ToVisit.pop()\n",
    "            \n",
    "        return res\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def GetUrlLinks(self, pageContent):\n",
    "        urls = re.findall('<a href=\"([^\"]+)\">', str(pageContent))\n",
    "        pattern = re.compile('https?')\n",
    "        urlsToReturn = set()\n",
    "        \n",
    "        for url in urls:\n",
    "            if pattern.match(url):\n",
    "                self.ToVisit.append(url)\n",
    "                urlsToReturn.add(url)\n",
    "        return urlsToReturn\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def Crawl(self, urls, depth = 30):\n",
    "        counter = 1\n",
    "        urlsLength = len(urls)\n",
    "        \n",
    "        for url in urls:\n",
    "            self.ToVisit.append(url)\n",
    "            print(\"Progress: \", (counter) * 100 / urlsLength, \" %\")\n",
    "            counter = counter + 1\n",
    "            while len(self.Visited) < depth and len(self.ToVisit) > 0:\n",
    "                currentUrl = self.GetCurrentUrlToFetch()\n",
    "                \n",
    "                if (currentUrl == \"\"):\n",
    "                    continue\n",
    "                \n",
    "                content = self.Fetch(currentUrl)\n",
    "                \n",
    "                if content == \"\":\n",
    "                    print(f\"Failed to fetch url: {currentUrl}\")\n",
    "                    continue\n",
    "                \n",
    "                self.Visited.add(currentUrl)\n",
    "                urls = self.GetUrlLinks(content)\n",
    "                text = self.TextFromHtml(content)\n",
    "                \n",
    "                Documents.append({\n",
    "                    \"url\": currentUrl,\n",
    "                    \"content\": text,\n",
    "                    \"urls\": urls\n",
    "                })\n",
    "                \n",
    "                print(\"Finished fetching url: \", currentUrl)\n",
    "                \n",
    "            self.ToVisit.clear()\n",
    "            self.Visited.clear()\n",
    "        \n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #\n",
    "\n",
    "# Ranker class for ranking documents\n",
    "class Ranker:\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.totalNodes = len(documents)\n",
    "        self.graph = self.GraphFromDocuments(documents)\n",
    "        self.pageRank = np.zeros(self.totalNodes)\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def PrintRanks(self):\n",
    "        for k in range(self.totalNodes):\n",
    "            print(\"Page rank of \", k, \" is :\\t\", self.pageRank[k])\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    # Retrieves the outgoing links for a document in a graph\n",
    "    def OutgoingLinks(self, nodeNumber):\n",
    "        return np.sum(self.graph[nodeNumber][:])\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "    \n",
    "    def GraphFromDocuments(self, documents):\n",
    "        documentsLength = len(documents)\n",
    "        \n",
    "        linksGraph = np.zeros((documentsLength, documentsLength))\n",
    "        \n",
    "        for i in range(documentsLength):\n",
    "            for j in range(documentsLength):\n",
    "                # if we are not on the same document\n",
    "                if (i != j):\n",
    "                    linksGraph[i][j] = documents[j][\"url\"] in documents[i][\"urls\"]\n",
    "                    \n",
    "        return linksGraph\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "    \n",
    "    def CalculateRanks(self):\n",
    "        \n",
    "        outgoingLinks = 0\n",
    "        dampingFactor = 0.85\n",
    "        tempPageRank = np.zeros(self.totalNodes)\n",
    "        \n",
    "        initialPageRank = 1.0 / self.totalNodes\n",
    "        #print(\"Total Number of nodes :\", self.totalNodes, \"\\t Initial pageRank of all nodes: \", initialPageRank, \"\\n\")\n",
    "        \n",
    "        # Initialization phase\n",
    "        for k in range(self.totalNodes):\n",
    "            self.pageRank[k] = initialPageRank\n",
    "            \n",
    "        #print(\"\\n Initial PageRank Values, Step 1\\n\")\n",
    "        \n",
    "        #self.PrintRanks()\n",
    "        \n",
    "        iterations = 10\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            # Store the PageRank for All Nodes in Temporary Array\n",
    "            for k in range(self.totalNodes):\n",
    "                tempPageRank[k] = self.pageRank[k]\n",
    "                self.pageRank[k] = 0\n",
    "                \n",
    "            for internalNodeNumber in range(self.totalNodes):\n",
    "                for externalNodeNumber in range(self.totalNodes):\n",
    "                    if (self.graph[externalNodeNumber][internalNodeNumber] == 1):\n",
    "                        outgoingLinks = self.OutgoingLinks(externalNodeNumber)\n",
    "\n",
    "                        # Calculate PageRank\n",
    "                        self.pageRank[internalNodeNumber] += tempPageRank[externalNodeNumber] * (1.0 / outgoingLinks)\n",
    "                        \n",
    "            #print(\"After step \", i, \" \\n\")\n",
    "            #self.PrintRanks()\n",
    "        return self.pageRank\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #            \n",
    "\n",
    "class FileManager:\n",
    "    \n",
    "    def Load(self, filename):\n",
    "        content = \"\"\n",
    "        with open(filename, 'r+', encoding= \"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "    \n",
    "    def Save(self, filename, content, append = False):\n",
    "        with open(filename, append == True and 'a' or 'w', encoding= \"utf-8\") as file:\n",
    "            file.write(content)\n",
    "            \n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "    def LoadAsCsv(self, filename):\n",
    "        return pd.read_csv(filename)\n",
    "\n",
    "# ---------------------------------------------------------- #\n",
    "\n",
    "    def SaveRankerModel(self, model, filename):\n",
    "        dump(model, filename)\n",
    "\n",
    "# ---------------------------------------------------------- #\n",
    "    \n",
    "    def LoadRankerModel(self, filename):\n",
    "        return load(filename)\n",
    "    \n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #\n",
    "\n",
    "class IndexBuilder:\n",
    "    \n",
    "    # Builds an inverted index using the passed documents\n",
    "    def BuildIndex(self, documents):\n",
    "        index = dict()\n",
    "        documentsLength = len(documents)\n",
    "\n",
    "        for i in range(documentsLength):\n",
    "            print(\"Progress: \", (i + 1) * 100 / documentsLength , \" %\")\n",
    "            for sentence in sent_tokenize(documents[i][\"content\"]):\n",
    "                for term in [enStemmer.stem(word.lower()) for word in word_tokenize(sentence) if word.lower() not in englishStopwords]:\n",
    "                    if term in index.keys():\n",
    "                        documentsList = index.get(term)\n",
    "                        if i not in documentsList:\n",
    "                            documentsList.append(i)\n",
    "                    else:\n",
    "                        documentsList = []\n",
    "                        documentsList.append(i)\n",
    "                    index.update({term: documentsList})\n",
    "        \n",
    "        return index\n",
    "    \n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #\n",
    "\n",
    "\n",
    "class TextAnalyzer:\n",
    "    \n",
    "    # Computes the frequency of a word in a content (document content or url)\n",
    "    def TFInContent(self, word, content, isUrl = False):\n",
    "        if isUrl == True:\n",
    "            words = self.WordsFromUrl(content)\n",
    "        else:\n",
    "            words = word_tokenize(content)\n",
    "            \n",
    "        frequency = [1 for term in words if term == word]\n",
    "        return len(frequency) / len(words)\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    # Computes the IDF of a word in contents (document's contents or urls)\n",
    "    def IDFInContents(self, word, contents, areUrls = False):\n",
    "        if areUrls == True:\n",
    "            chunks = [self.WordsFromUrl(url) for url in contents]\n",
    "        else:\n",
    "            chunks = [sentence for content in contents for sentence in sent_tokenize(content)]\n",
    "\n",
    "        counter = 1\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            if word in chunk:\n",
    "                counter += 1\n",
    "\n",
    "        return np.log(len(chunks) / counter)\n",
    "        \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def WordsFromUrl(self, url):\n",
    "        words = url.split(\"//\")[1].split(\"/\")\n",
    "        wordsInDomain = words[0].split(\".\")\n",
    "        words.append(wordsInDomain[\"www\" in wordsInDomain and 1 or 0])\n",
    "        del words[0]\n",
    "        \n",
    "        return words\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "    \n",
    "    def ExtractInfoForModel(self, query, documentsResults, forPrediction = True):\n",
    "        info = list()\n",
    "        \n",
    "        if len(documentsResults) < 1:\n",
    "            return info\n",
    "        \n",
    "        # Get all urls returned\n",
    "        urls = [docResult.document[\"url\"] for docResult in documentsResults]\n",
    "        \n",
    "        # Get the content of all the documents returned\n",
    "        contents = [docResult.document[\"content\"] for docResult in documentsResults]\n",
    "        \n",
    "        # Loop over all results and get the info required for the model\n",
    "        for result in documentsResults:\n",
    "            doc_tf = self.TFInContent(query, result.document[\"content\"], isUrl = False)\n",
    "            url_tf = self.TFInContent(query, result.document[\"url\"], isUrl = True)\n",
    "            doc_idf = self.IDFInContents(query, contents, areUrls = False)\n",
    "            url_idf = self.IDFInContents(query, urls, areUrls = True)\n",
    "            url_length = len(result.document[\"url\"])\n",
    "            doc_length = len(result.document[\"content\"])\n",
    "            page_rank = result.rank\n",
    "            outgoing_links = len(result.document[\"urls\"])\n",
    "            \n",
    "            if forPrediction == True:\n",
    "                info.append({\n",
    "                \"query\": self.Hash(query),\n",
    "                \"doc_tf\": doc_tf,\n",
    "                \"url_tf\": url_tf,\n",
    "                \"doc_idf\": doc_idf,\n",
    "                \"url_idf\": url_idf,\n",
    "                \"doc_tf-idf\": doc_tf * doc_idf,\n",
    "                \"url_tf-idf\": url_tf * url_idf,\n",
    "                \"url_length\": url_length,\n",
    "                \"doc_length\": doc_length,\n",
    "                \"outgoing_links\": outgoing_links\n",
    "                })\n",
    "                \n",
    "            else:\n",
    "                # Add info extracted\n",
    "                ModelInfo.append({\n",
    "                    \"query\": query,\n",
    "                    \"doc_tf\": doc_tf,\n",
    "                    \"url_tf\": url_tf,\n",
    "                    \"doc_idf\": doc_idf,\n",
    "                    \"url_idf\": url_idf,\n",
    "                    \"doc_tf-idf\": doc_tf * doc_idf,\n",
    "                    \"url_tf-idf\": url_tf * url_idf,\n",
    "                    \"url_length\": url_length,\n",
    "                    \"doc_length\": doc_length,\n",
    "                    \"page_rank\": page_rank,\n",
    "                    \"outgoing_links\": outgoing_links\n",
    "                })\n",
    "        \n",
    "        if forPrediction == True:\n",
    "            return Helper().CreateDataFrame(info, forPrediction = True)\n",
    "        \n",
    "        print(\"Done extracting info of query: \", query)\n",
    "        \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def Hash(self, text):\n",
    "        return int(hashlib.sha256(text.encode('utf-8')).hexdigest(), 32) % 10**5\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    # Retrieves the difference between two words as a number of operations (edits, inserts, updates)\n",
    "    # Minimum Edit Distance\n",
    "    def MinEditDistance(self, word1, word2):\n",
    "        res = np.zeros((len(word1) + 1, len(word2) + 1))\n",
    "\n",
    "        for i in range(len(word1) + 1):\n",
    "            res[i,0] = i\n",
    "\n",
    "        for j in range(len(word2) + 1):\n",
    "            res[0,j] = j\n",
    "\n",
    "        for i in range(1, len(word1) + 1):\n",
    "            for j in range(1, len(word2) + 1):\n",
    "\n",
    "                c1 = res[i, j - 1] + 1\n",
    "                c2 = res[i - 1, j] + 1\n",
    "\n",
    "                if (word1[i - 1] == word2[j - 1]):\n",
    "                    c3 = res[i - 1, j - 1]\n",
    "                else:\n",
    "                    c3 = res[i - 1, j - 1] + 2\n",
    "\n",
    "                res[i,j] = min(c1, c2, c3)\n",
    "\n",
    "        return res[len(word1), len(word2)]\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "    \n",
    "    def PredictWordsFromQuery(self, query, limit = 5):\n",
    "        words = Index.keys()\n",
    "        similaritiesList = list()\n",
    "        for word in words:\n",
    "            similaritiesList.append({\n",
    "                'word': word,\n",
    "                'similarity': self.MinEditDistance(query, word)\n",
    "            })\n",
    "            \n",
    "        similaritiesList = sorted(similaritiesList, key= lambda x: x['similarity'])\n",
    "        \n",
    "        return [similaritiesList[i]['word'] for i in range(limit)]\n",
    "        \n",
    "    \n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #\n",
    "\n",
    "class Searcher:\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def __init__(self, index, documentsLength):\n",
    "        self.index = index\n",
    "        self.documentsLength = documentsLength\n",
    "\n",
    "    # Does a search\n",
    "    # Query gonna be:\n",
    "    #   1- And: query1 and query2 and query3... document that exists in all three query's results\n",
    "    #   2- Not: query1 not query2 not query3... document that exists only in the first query results\n",
    "    #   3- Or: query1 or query2 or query3... documents of all queries results combined without redundancy\n",
    "    def Search(self, query, printResults = False, useRankerModel = True):\n",
    "        \n",
    "        # get current time in seconds\n",
    "        startTime = time.time()\n",
    "        \n",
    "        results = list()\n",
    "        if (\" and \" in query):\n",
    "            results = self.AndSearch(query)\n",
    "        elif (\" not \" in query):\n",
    "            results = self.NotSearch(query)\n",
    "        else:\n",
    "            # Get the synonyms and do the search including those synonyms\n",
    "            modifiedQuery = query\n",
    "            for synonym in self.SynonymsForWordInQuery(query):\n",
    "                modifiedQuery += f\" {synonym}\"\n",
    "                \n",
    "            results = self.OrSearch(modifiedQuery)\n",
    "        \n",
    "        documentsResults = [Documents[index] for index in results]\n",
    "        \n",
    "        if (len(documentsResults) < 1):\n",
    "                print(f\"Results: {len(documentsResults)} ({time.time() - startTime:.2f} seconds)\")\n",
    "                return\n",
    "        \n",
    "        rankings = list()\n",
    "        \n",
    "        if (useRankerModel == True):\n",
    "            model = FileManager().LoadRankerModel(trainedModelPath)\n",
    "            predictionInfo = TextAnalyzer().ExtractInfoForModel(query, list([DocumentResult(documentsResults[i]) for i in range(len(documentsResults))]), forPrediction = True)\n",
    "            rankings = list(model.predict(predictionInfo))\n",
    "        else:\n",
    "            rankings = Ranker(documentsResults).CalculateRanks()\n",
    "\n",
    "        finalResults = [DocumentResult(documentsResults[i], rankings[i]) for i in range(len(rankings))]\n",
    "\n",
    "        finalResults.sort(key= lambda o: o.rank, reverse = True)\n",
    "        \n",
    "        # Send the results for extracting info to train the ranker model\n",
    "        #TextAnalyzer().ExtractInfoForModel(query, finalResults)\n",
    "\n",
    "        print(f\"Results: {len(finalResults)} ({time.time() - startTime:.2f} seconds)\")\n",
    "        \n",
    "        # Print the results\n",
    "        if (printResults):\n",
    "            for result in finalResults:\n",
    "                  print(result.document[\"url\"], \" With Rank: \", result.rank)\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    # Retrieves the documents for a spcific term\n",
    "    def TermSearch(self, term):\n",
    "        try:\n",
    "            return self.index[enStemmer.stem(term.lower())]\n",
    "        except KeyError:\n",
    "            return list()\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    # Does a not search for a specific term\n",
    "    def NotSearch(self, query):\n",
    "        finalResults = list()\n",
    "        results = list()\n",
    "        \n",
    "        # foreach stemmed lowered non-stopword term from the query splitted by 'not'\n",
    "        for term in [enStemmer.stem(token.lower()) for token in query.split(' not ') if token.lower() not in englishStopwords]:\n",
    "            docsResults = self.TermSearch(token)\n",
    "            if len(docsResults) > 0:\n",
    "                results.append(docsResults)\n",
    "        \n",
    "         # if there are results\n",
    "        if len(results) > 0 :\n",
    "            \n",
    "            # take arrays after the first one.. from 1 to the end\n",
    "            otherDocsResults = results[1:]\n",
    "\n",
    "            # foreach document in the first results\n",
    "            for document in results[0]:\n",
    "\n",
    "                # indicator to check if the document exists in other documents\n",
    "                existsInOtherDocs = False\n",
    "\n",
    "                # check if the current document is in all other documents\n",
    "                for otherDocs in otherDocsResults:\n",
    "                    existsInOtherDocs = document in otherDocs\n",
    "                    # get out if the document doesn't exist in the current set of documents\n",
    "                    if existsInOtherDocs == True:\n",
    "                        break\n",
    "\n",
    "                # if it occured in all other array of docs, add it\n",
    "                if existsInOtherDocs == False:\n",
    "                    finalResults.add(document)\n",
    "\n",
    "        return finalResults\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    # Retrieves results for an OR search\n",
    "    def OrSearch(self, text):\n",
    "        results  = set()\n",
    "        \n",
    "        for term in [enStemmer.stem(word.lower()) for word in text.split() if word.lower() not in englishStopwords]:\n",
    "            for documentIndex in self.TermSearch(term):\n",
    "                results.add(documentIndex)\n",
    "\n",
    "        return results\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    # Retrieves results for an AND search\n",
    "    def AndSearch(self, text):\n",
    "        # Inital search results for each token\n",
    "        results = set()\n",
    "\n",
    "        # Final results to be returned\n",
    "        finalResults = set()\n",
    "\n",
    "        # foreach stemmed lowered non stopword word in the text passed splitted by 'and'\n",
    "        for token in [enStemmer.stem(word.lower()) for word in text.split(\" and \") if word.lower() not in englishStopwords]:\n",
    "            # Do a normal search for the word\n",
    "            docsResults = self.TermSearch(token)\n",
    "            if len(docsResults) > 0:\n",
    "                results.add(docsResults)\n",
    "\n",
    "        # if there are results\n",
    "        if len(results) > 0 :\n",
    "            # sort the results to work with the smallest array first\n",
    "            results.sort()\n",
    "\n",
    "            # take arrays after the first one.. from 1 to the end\n",
    "            otherDocsResults = results[1:]\n",
    "\n",
    "            # foreach document in the first results\n",
    "            for document in results[0]:\n",
    "\n",
    "                # indicator to check if the document exists in all other documents\n",
    "                goodToAdd = False\n",
    "\n",
    "                # check if the current document is in all other documents\n",
    "                for otherDocs in otherDocsResults:\n",
    "                    goodToAdd = document in otherDocs\n",
    "                    # get out if the document doesn't exist in the current set of documents\n",
    "                    if goodToAdd == False:\n",
    "                        break;\n",
    "\n",
    "                # if it occured in all other array of docs, add it\n",
    "                if goodToAdd == True:\n",
    "                    finalResults.add(document)\n",
    "\n",
    "        return finalResults\n",
    "    \n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "    def SynonymsForWordInQuery(self, query):\n",
    "        syn = set()\n",
    "        ant = list()\n",
    "        indexWords = Index.keys()\n",
    "        for word in query.split():\n",
    "            for synset in wn.synsets(word):\n",
    "                for lemma in synset.lemmas():\n",
    "                    name = lemma.name()\n",
    "                    if name in indexWords and name != word:\n",
    "                        syn.add(enStemmer.stem(name))\n",
    "        return syn\n",
    "        \n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #\n",
    "\n",
    "class ModelTrainer:\n",
    "    \n",
    "    def __init__(self, dataFrame):\n",
    "        self.dataFrame = dataFrame\n",
    "    \n",
    "    def Hash(self, text):\n",
    "        return int(hashlib.sha256(text.encode('utf-8')).hexdigest(), 32) % 10**5\n",
    "    \n",
    "    def TrainModel(self, trianingColumnName):\n",
    "        X = dataFrame.loc[:, dataFrame.columns != trianingColumnName]\n",
    "        y = dataFrame[trianingColumnName]\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            X.iat[i, 0] = self.Hash(X.iat[i, 0])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "        return LinearRegression().fit(X_train, y_train)\n",
    "        \n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% #\n",
    "        \n",
    "class DocumentResult:\n",
    "    \n",
    "    def __init__(self, document, rank = 0.0):\n",
    "        self.document = document\n",
    "        self.rank = rank\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.rank < other.rank\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return self.rank == other.rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.healthline.com/nutrition/11-most-nutrient-dense-foods-on-the-planet\",\n",
    "    \"https://www.myfooddiary.com/foods\",\n",
    "    \"https://www.epicurious.com/search\",\n",
    "    \"https://www.tasteofhome.com/collection/the-best-soup-recipes\",\n",
    "    \"https://www.britannica.com/plant/tomato\",\n",
    "    \"https://www.healthline.com/nutrition/foods/tomatoes\",\n",
    "    \"https://www.britannica.com/plant/potato\",\n",
    "    \"https://www.livescience.com/45838-potato-nutrition.html\",\n",
    "    \"https://www.vegetables.co.nz/vegetables-a-z\",\n",
    "    \"https://www.choosemyplate.gov/eathealthy/fruits\",\n",
    "    \"https://www.fruitatwork.com.au/how-it-works/fruit-information\",\n",
    "    \"https://www.healthyeating.org/healthy-eating/all-star-foods/fruits\",\n",
    "    \"https://www.food.com\",\n",
    "    \"https://foodnetwork.co.uk/?utm_source=foodnetwork.com&utm_medium=domestic\",\n",
    "    \"https://www.nytimes.com/section/food\",\n",
    "    \"https://www.eatthis.com/classic-comfort-foods-no-longer-eaten\",\n",
    "    \"https://www.eatright.org/food\",\n",
    "    \"https://www.takingcharge.csh.umn.edu/how-does-food-impact-health\",\n",
    "    \"https://www.nutrition.gov/topics/whats-food\",\n",
    "    \"http://foodhealthlegal.com/?p=140\",\n",
    "    \"https://www.disabled-world.com/fitness\",\n",
    "    \"https://www.nhs.uk/live-well\",\n",
    "    \"https://www.rbkc.gov.uk/business-and-enterprise/regulation/food-safety/food-information-regulations\",\n",
    "    \"https://informationisbeautiful.net/subject/food/\",\n",
    "    \"https://www.fsai.ie/\",\n",
    "    \"https://www.cdc.gov/healthyweight/\",\n",
    "    \"https://health.hawaii.gov/san/food-information/\",\n",
    "    \"https://www.sfa.gov.sg/food-information\"\n",
    "]\n",
    "\n",
    "#Crawler().Crawl(urls, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food', 'foot', 'floor', 'f', 'foodi']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User enters a query and the system gets all words close to the word he typed\n",
    "TextAnalyzer().PredictWordsFromQuery('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 71 (15.49 seconds)\n",
      "http://ec.europa.eu/food/safety/novel_food/catalogue/search/public/index.cfm  With Rank:  0.01516819216241292\n",
      "https://www.food.com/ideas/top-grilling-recipes-6974#c-810376  With Rank:  0.014554780403812449\n",
      "https://health.hawaii.gov/san/food-information/  With Rank:  0.014545448901118864\n",
      "https://www.eatthis.com/advertising-opportunities/  With Rank:  0.014114827229359016\n",
      "https://www.fruitatwork.com.au/how-it-works/fruit-delivery-sunshine-coast  With Rank:  0.013755141818964891\n",
      "https://www.eatthis.com/partner-sites/  With Rank:  0.013671311667667975\n",
      "https://www.rbkc.gov.uk/business-and-enterprise/regulation/food-safety/food-information-regulations  With Rank:  0.01354402992687983\n",
      "https://www.eatthis.com/classic-comfort-foods-no-longer-eaten  With Rank:  0.013332348295211115\n",
      "https://www.fruitatwork.com.au/how-it-works/fruit-delivery-toowoomba  With Rank:  0.013235977479194708\n",
      "https://www.fruitatwork.com.au/how-it-works/fruit-delivery-penrith  With Rank:  0.013066200712131874\n",
      "https://ec.europa.eu/food/safety/novel_food/catalogue_en  With Rank:  0.013002345907866152\n",
      "https://www.fruitatwork.com.au/how-it-works/fruit-delivery-perth  With Rank:  0.012931749320463119\n",
      "https://itunes.apple.com/us/app/eat-this-not-that-the-game/id361436133?mt=8  With Rank:  0.012697026958235762\n",
      "http://ec.europa.eu/food/safety/labelling_nutrition/labelling_legislation/index_en.htm  With Rank:  0.01264110319520864\n",
      "https://www.eatthis.com/podcast/  With Rank:  0.012531505339975654\n",
      "https://www.livescience.com/why-covid-19-coronavirus-deadly-for-some-people.html  With Rank:  0.01220118013544029\n",
      "http://foodhealthlegal.com/?p=140  With Rank:  0.012052609368961525\n",
      "https://informationisbeautiful.net/blog  With Rank:  0.011988439422362926\n",
      "https://www.eatthis.com/contact-us/  With Rank:  0.011980188319671611\n",
      "https://www.fruitatwork.com.au/how-it-works/fruit-information  With Rank:  0.011686318100211947\n",
      "https://www.eatthis.com/faq/  With Rank:  0.01114498965727447\n",
      "https://informationisbeautiful.net/subject/food/  With Rank:  0.0111140059324693\n",
      "http://foodhealthlegal.com/?p=1028  With Rank:  0.010925322869184547\n",
      "https://www.nutrition.gov/topics/whats-food  With Rank:  0.010848925629768797\n",
      "https://www.healthyeating.org/healthy-eating/all-star-foods/fruits  With Rank:  0.010754437916466021\n",
      "https://www.cdc.gov/healthyweight/  With Rank:  0.0106395993501825\n",
      "https://www.takingcharge.csh.umn.edu/how-does-food-impact-health  With Rank:  0.010614204600867965\n",
      "http://foodhealthlegal.com/?p=1003  With Rank:  0.010537905533985302\n",
      "http://foodhealthlegal.com/?p=1036  With Rank:  0.01051048622209965\n",
      "https://www.eatthis.com/newsletters/  With Rank:  0.010459200135185026\n",
      "http://foodhealthlegal.com/?p=1024  With Rank:  0.010414771197077748\n",
      "https://www.takingcharge.csh.umn.edu/guided-imagery  With Rank:  0.01037021830282482\n",
      "https://www.ssa.gov/ssi/spotlights/spot-loans.htm  With Rank:  0.010337744842464025\n",
      "http://food.gov.uk/business-industry/guidancenotes/allergy-guide/  With Rank:  0.010318158459921303\n",
      "https://www.disabled-world.com/disability/finance/  With Rank:  0.010135816900221709\n",
      "https://www.disabled-world.com/fitness/nutrition/fruits-veggies/colored.php  With Rank:  0.009973038450651832\n",
      "http://foodhealthlegal.com/?p=1011  With Rank:  0.009880641268270762\n",
      "https://www.healthline.com/nutrition/11-most-nutrient-dense-foods-on-the-planet  With Rank:  0.009783144065088413\n",
      "https://www.britannica.com/topic/chuno  With Rank:  0.009752389475257982\n",
      "https://www.choosemyplate.gov/eathealthy/fruits  With Rank:  0.009612576522785223\n",
      "https://www.myfooddiary.com/foods  With Rank:  0.009548837113501959\n",
      "https://www.fsai.ie/  With Rank:  0.009501696049135188\n",
      "https://www.eatthis.com/privacy-policy/  With Rank:  0.00935409280669222\n",
      "https://www.britannica.com/plant/potato  With Rank:  0.009346999662550329\n",
      "https://www.sfa.gov.sg/food-information  With Rank:  0.009321889333433437\n",
      "https://www.britannica.com/topic/fruit-food  With Rank:  0.009066601131132283\n",
      "https://ec.europa.eu/food/safety/rasff/country-fact-sheets_en  With Rank:  0.008955044350299174\n",
      "https://www.disabled-world.com/fitness/  With Rank:  0.008951791587963925\n",
      "https://www.nhs.uk/live-well  With Rank:  0.008910087662927927\n",
      "https://www.disabled-world.com/fitness  With Rank:  0.008871905931005657\n",
      "https://ec.europa.eu/food/safety/rasff/portal_en  With Rank:  0.008830124519544354\n",
      "https://usda.gov  With Rank:  0.008733288351934687\n",
      "https://www.livescience.com/45838-potato-nutrition.html  With Rank:  0.008601023545374578\n",
      "https://www.ssa.gov/ssi/text-expedite-ussi.htm  With Rank:  0.008451553212669324\n",
      "https://www.disabled-world.com/fitness/nutrition/fruits-veggies/rss.xml  With Rank:  0.008384289998049662\n",
      "https://www.healthline.com/nutrition/foods/tomatoes  With Rank:  0.008264943225339346\n",
      "https://www.britannica.com/plant/tomato  With Rank:  0.008258024088675674\n",
      "https://www.epicurious.com/search  With Rank:  0.008149809530267288\n",
      "https://ec.europa.eu/food/safety_en  With Rank:  0.008097789913549744\n",
      "https://www.food.com/recipe/mermaid-animal-cake-popsicles-536230  With Rank:  0.006707850979877146\n",
      "https://www.eatthis.com/terms-and-conditions/  With Rank:  0.005958929008789844\n",
      "https://www.vegetables.co.nz/vegetables-a-z  With Rank:  0.005620473581613775\n",
      "https://www.usa.gov/  With Rank:  0.0050522056159386045\n",
      "https://www.usa.gov/  With Rank:  0.0050522056159386045\n",
      "https://www.usa.gov/  With Rank:  0.0050522056159386045\n",
      "https://www.disabled-world.com/fitness/rss.xml  With Rank:  0.0049976823161092965\n",
      "https://www.nytimes.com/section/food  With Rank:  0.004635737111467702\n",
      "https://www.eatright.org/food  With Rank:  0.003130954159608926\n",
      "https://www.disabled-world.com/disability/finance/rss.xml  With Rank:  0.003049366480523533\n",
      "https://www.food.com  With Rank:  -0.0020422524583042587\n",
      "https://foodnetwork.co.uk/?utm_source=foodnetwork.com&utm_medium=domestic  With Rank:  -0.004632854768651988\n"
     ]
    }
   ],
   "source": [
    "# User chooses from the words infront of him and does a search\n",
    "Searcher(Index, len(Documents)).Search(\"food\", printResults = True, useRankerModel = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
